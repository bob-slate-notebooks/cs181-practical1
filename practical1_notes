neural network
- adaptive basis function regression
- for linear, we had a phi that we multiplied with w, then find gradient 
- instead of just one phi, 
scikitlearn
pandas
numpy

cross validation - divide up training data and set lambda

higher alpha - regularization parameter - more of your weights are gonna be zero
lambda is the same thing as alpha

rdkit
http://rdkit.org/UGM/2012/Landrum_RDKit_UGM.Fingerprints.Final.pptx.pdf


GetMorganFingerprintAsBitVect( (Mol)mol, (int)radius [, (int)nBits=2048 [, (AtomPairsParameters)invariants=[] [, (AtomPairsParameters)fromAtoms=[] [, (bool)useChirality=False [, (bool)useBondTypes=True [, (bool)useFeatures=False [, (AtomPairsParameters)bitInfo=None]]]]]]]) -> ExplicitBitVect :


plot training and test and see when overfitting
higher alpha values means more penalty
